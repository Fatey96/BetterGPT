{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYLkk3CBkWCtjS71qm06p9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatey96/BetterGPT/blob/main/eegvit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class EEGEyeNetDataset(Dataset):\n",
        "    def __init__(self, data_file,transpose = True):\n",
        "        self.data_file = data_file\n",
        "        print('loading data...')\n",
        "        with np.load(self.data_file) as f: # Load the data array\n",
        "            self.trainX = f['EEG']\n",
        "            self.trainY = f['labels']\n",
        "        print(self.trainY)\n",
        "        if transpose:\n",
        "            self.trainX = np.transpose(self.trainX, (0,2,1))[:,np.newaxis,:,:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read a single sample of data from the data array\n",
        "        X = torch.from_numpy(self.trainX[index]).float()\n",
        "        y = torch.from_numpy(self.trainY[index,1:3]).float()\n",
        "        # Return the tensor data\n",
        "        return (X,y,index)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Compute the number of samples in the data array\n",
        "        return len(self.trainX)"
      ],
      "metadata": {
        "id": "VhSYWb3MP2fO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"./dataset/Position_task_with_dots_synchronised_min.npz\" \"https://osf.io/download/ge87t/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWXAJWDVB39u",
        "outputId": "46c76a9b-6212-4dea-c548-5fc10276aaac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-27 03:10:46--  https://osf.io/download/ge87t/\n",
            "Resolving osf.io (osf.io)... 35.190.84.173\n",
            "Connecting to osf.io (osf.io)|35.190.84.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://files.osf.io/v1/resources/ktv7m/providers/dropbox/prepared/Position_task_with_dots_synchronised_min.npz [following]\n",
            "--2023-12-27 03:10:48--  https://files.osf.io/v1/resources/ktv7m/providers/dropbox/prepared/Position_task_with_dots_synchronised_min.npz\n",
            "Resolving files.osf.io (files.osf.io)... 35.186.214.196\n",
            "Connecting to files.osf.io (files.osf.io)|35.186.214.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11075939748 (10G) [application/octet-stream]\n",
            "Saving to: ‘./dataset/Position_task_with_dots_synchronised_min.npz’\n",
            "\n",
            "./dataset/Position_ 100%[===================>]  10.31G  8.06MB/s    in 20m 16s \n",
            "\n",
            "2023-12-27 03:31:08 (8.68 MB/s) - ‘./dataset/Position_task_with_dots_synchronised_min.npz’ saved [11075939748/11075939748]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7DCDfxp0Bp3Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import ViTModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "\n",
        "class EEGViT_pretrained(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=256,\n",
        "            kernel_size=(1, 36),\n",
        "            stride=(1, 36),\n",
        "            padding=(0,2),\n",
        "            bias=False\n",
        "        )\n",
        "        self.batchnorm1 = nn.BatchNorm2d(256, False)\n",
        "        model_name = \"google/vit-base-patch16-224\"\n",
        "        config = transformers.ViTConfig.from_pretrained(model_name)\n",
        "        config.update({'num_channels': 256})\n",
        "        config.update({'image_size': (129,14)})\n",
        "        config.update({'patch_size': (8,1)})\n",
        "\n",
        "        model = transformers.ViTForImageClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
        "        model.vit.embeddings.patch_embeddings.projection = torch.nn.Conv2d(256, 768, kernel_size=(8, 1), stride=(8, 1), padding=(0,0), groups=256)\n",
        "        model.classifier=torch.nn.Sequential(torch.nn.Linear(768,1000,bias=True),\n",
        "                                     torch.nn.Dropout(p=0.1),\n",
        "                                     torch.nn.Linear(1000,2,bias=True))\n",
        "        self.ViT = model\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.batchnorm1(x)\n",
        "        x=self.ViT.forward(x).logits\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import ViTModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "\n",
        "class EEGViT_raw(nn.Module):\n",
        "    def __init__(self, ViTLayers):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=256,\n",
        "            kernel_size=(1, 36),\n",
        "            stride=(1, 36),\n",
        "            padding=(0,2),\n",
        "            bias=False\n",
        "        )\n",
        "        self.batchnorm1 = nn.BatchNorm2d(256, False)\n",
        "        config = transformers.ViTConfig(\n",
        "            hidden_size=768,\n",
        "            num_hidden_layers=12,\n",
        "            num_attention_heads=12,\n",
        "            intermediate_size=3072,\n",
        "            hidden_dropout_prob=0.1,\n",
        "            attention_probs_dropout_prob=0.1,\n",
        "            initializer_range=0.02,\n",
        "            num_channels=256,\n",
        "            image_size=(129,14),\n",
        "            patch_size=(8,1)\n",
        "        )\n",
        "        model = ViTModel(config)\n",
        "        model.embeddings.patch_embeddings.projection = torch.nn.Conv2d(256, 768, kernel_size=(8, 1), stride=(8, 1), padding=(0,0), groups=256)\n",
        "        model.pooler.activation = torch.nn.Sequential(torch.nn.Dropout(p=0.1),\n",
        "                                                       torch.nn.Linear(768,2,bias=True))\n",
        "        self.ViT = model\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.batchnorm1(x)\n",
        "        x=self.ViT(x).pooler_output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZzulKCldCGKz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import ViTModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "\n",
        "class ViTBase_pretrained(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model_name = \"google/vit-base-patch16-224\"\n",
        "        config = transformers.ViTConfig.from_pretrained(model_name)\n",
        "        config.update({'num_channels': 1})\n",
        "        config.update({'image_size': (129,500)})\n",
        "        config.update({'patch_size': (8,35)})\n",
        "\n",
        "        model = transformers.ViTForImageClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
        "        model.vit.embeddings.patch_embeddings.projection = nn.Sequential(torch.nn.Conv2d(1, 768, kernel_size=(8, 36), stride=(8, 36), padding=(0,2)),\n",
        "                                                                        nn.BatchNorm2d(768))\n",
        "        model.classifier=torch.nn.Sequential(torch.nn.Linear(768,1000,bias=True),\n",
        "                                     torch.nn.Dropout(p=0.1),\n",
        "                                     torch.nn.Linear(1000,2,bias=True))\n",
        "        self.ViT = model\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.ViT(x).logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "2vf2pAPVCyrU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import ViTModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "\n",
        "class ViTBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        config = transformers.ViTConfig(\n",
        "            hidden_size=768,\n",
        "            num_hidden_layers=12,\n",
        "            num_attention_heads=12,\n",
        "            intermediate_size=3072,\n",
        "            hidden_dropout_prob=0.1,\n",
        "            attention_probs_dropout_prob=0.1,\n",
        "            initializer_range=0.02,\n",
        "            num_channels=1,\n",
        "            image_size=(129,500),\n",
        "            patch_size=(8,35)\n",
        "        )\n",
        "        model = ViTModel(config)\n",
        "        model.embeddings.patch_embeddings.projection = torch.nn.Conv2d(1, 768, kernel_size=(8, 36), stride=(8, 36), padding=(0,2))\n",
        "        model.pooler.activation = torch.nn.Sequential(torch.nn.Dropout(p=0.1),\n",
        "                                                    torch.nn.Linear(768,2,bias=True))\n",
        "    def forward(self,x):\n",
        "        x=self.model(x).pooler_output\n",
        "        return x"
      ],
      "metadata": {
        "id": "MybYnnuwC78R"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def split(ids, train, val, test):\n",
        "    # proportions of train, val, test\n",
        "    assert (train+val+test == 1)\n",
        "    IDs = np.unique(ids)\n",
        "    num_ids = len(IDs)\n",
        "\n",
        "    # priority given to the test/val sets\n",
        "    test_split = math.ceil(test * num_ids)\n",
        "    val_split = math.ceil(val * num_ids)\n",
        "    train_split = num_ids - val_split - test_split\n",
        "\n",
        "    train = np.where(np.isin(ids, IDs[:train_split]))[0]\n",
        "    val = np.where(np.isin(ids, IDs[train_split:train_split+val_split]))[0]\n",
        "    test = np.where(np.isin(ids, IDs[train_split+val_split:]))[0]\n",
        "\n",
        "    return train, val, test"
      ],
      "metadata": {
        "id": "u-3Q8qTpDA9l"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "models: EEGViT_pretrained; EEGViT_raw; ViTBase; ViTBase_pretrained\n",
        "'''\n",
        "model = EEGViT_pretrained()\n",
        "EEGEyeNet = EEGEyeNetDataset('./dataset/Position_task_with_dots_synchronised_min.npz')\n",
        "batch_size = 64\n",
        "n_epoch = 15\n",
        "learning_rate = 1e-4\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "def train(model, optimizer, criterion, scheduler=None):\n",
        "    '''\n",
        "        model: model to train\n",
        "        optimizer: optimizer to update weights\n",
        "        criterion: loss function\n",
        "        scheduler: scheduling learning rate, used when finetuning pretrained models\n",
        "    '''\n",
        "    torch.cuda.empty_cache()\n",
        "    train_indices, val_indices, test_indices = split(EEGEyeNet.trainY[:, 0], 0.7, 0.15, 0.15)  # indices for the training set\n",
        "    print('create dataloader...')\n",
        "\n",
        "    train = Subset(EEGEyeNet, indices=train_indices)\n",
        "    val = Subset(EEGEyeNet, indices=val_indices)\n",
        "    test = Subset(EEGEyeNet, indices=test_indices)\n",
        "\n",
        "    train_loader = DataLoader(train, batch_size=batch_size)\n",
        "    val_loader = DataLoader(val, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test, batch_size=batch_size)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_id = 0  # Change this to the desired GPU ID if you have multiple GPUs\n",
        "        torch.cuda.set_device(gpu_id)\n",
        "        device = torch.device(f\"cuda:{gpu_id}\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)  # Wrap the model with DataParallel\n",
        "    print(\"HI\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Initialize lists to store losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    print('training...')\n",
        "    # Train the model\n",
        "    for epoch in range(n_epoch):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "\n",
        "        for i, (inputs, targets, index) in tqdm(enumerate(train_loader)):\n",
        "            # Move the inputs and targets to the GPU (if available)\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Compute the outputs and loss for the current batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "\n",
        "            # Compute the gradients and update the parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Print the loss and accuracy for the current batch\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
        "\n",
        "        epoch_train_loss /= len(train_loader)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            for inputs, targets, index in val_loader:\n",
        "                # Move the inputs and targets to the GPU (if available)\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Compute the outputs and loss for the current batch\n",
        "                outputs = model(inputs)\n",
        "                # print(outputs)\n",
        "                loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch}, Val Loss: {val_loss}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            for inputs, targets, index in test_loader:\n",
        "                # Move the inputs and targets to the GPU (if available)\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Compute the outputs and loss for the current batch\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_loss /= len(test_loader)\n",
        "            test_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch}, test Loss: {val_loss}\")\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(model, optimizer=optimizer, criterion=criterion, scheduler=scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "LQ06t3WTDFa3",
        "outputId": "002fbddd-509c-4352-d609-ca8a3e00bdc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- vit.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 256, 8, 1]) in the model instantiated\n",
            "- vit.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 225, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-aff748cb6478>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m '''\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGViT_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mEEGEyeNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEGEyeNetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/Position_task_with_dots_synchronised_min.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-88bfb1b836e3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_file, transpose)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Load the data array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EEG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    814\u001b[0m                                                              count=read_count)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    995\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_STORED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    746\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAt0FY7jXx7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cl40aKjSOyAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}